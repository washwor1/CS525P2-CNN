{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFor this entire file there are a few constants:\\nactivation:\\n0 - linear\\n1 - logistic (only one supported)\\nloss:\\n0 - sum of square errors\\n1 - binary cross entropy\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "\"\"\"\n",
    "For this entire file there are a few constants:\n",
    "activation:\n",
    "0 - linear\n",
    "1 - logistic (only one supported)\n",
    "loss:\n",
    "0 - sum of square errors\n",
    "1 - binary cross entropy\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuron class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    #initilize neuron with activation type, number of inputs, learning rate, and possibly with set weights\n",
    "    def __init__(self,activation, input_num, lr, weights=None):\n",
    "        self.func = activation\n",
    "        self.value = input_num\n",
    "        self.learning_rate = lr\n",
    "        \n",
    "        \n",
    "        if weights is not None:\n",
    "            self.weights = weights\n",
    "            \n",
    "        else:\n",
    "            self.weights = np.random.random()    \n",
    "        \n",
    "        self.weights_no_bias = np.delete(self.weights, -1)\n",
    "    #This method returns the activation of the net\n",
    "    def activate(self, net):\n",
    "        if self.func == 1:\n",
    "            return 1 / (1 + np.exp(-net))\n",
    "        \n",
    "        elif self.func == 0:\n",
    "            return net\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Invalid activation function')      \n",
    "        \n",
    "    #Calculate the output of the neuron should save the input and output for back-propagation.   \n",
    "    def calculate(self,input):\n",
    "        self.inputs = np.append(input, 1)\n",
    "        self.net = np.dot(self.inputs, self.weights)\n",
    "        self.output = self.activate(self.net)\n",
    "        self.inputs = np.delete(self.inputs, -1)\n",
    "        return self.output\n",
    "\n",
    "    #This method returns the derivative of the activation function with respect to the net   \n",
    "    def activationderivative(self):\n",
    "        if self.func == 1:\n",
    "            return self.output * (1 - self.output)\n",
    "        \n",
    "        elif self.func == 0:\n",
    "            return 1\n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Invalid activation function')\n",
    "\n",
    "          \n",
    "    \n",
    "    #This method calculates the partial derivative for each weight and returns the delta*w to be used in the previous layer\n",
    "    def calcpartialderivative(self, wtimesdelta):\n",
    "        self.delta = wtimesdelta * self.activationderivative()\n",
    "        self.partial_derivatives = np.dot(np.transpose(self.inputs), self.delta)\n",
    "        return np.dot(self.delta, self.weights_no_bias)\n",
    "\n",
    "\n",
    "         \n",
    "    \n",
    "    #Simply update the weights using the partial derivatives and the leranring weight\n",
    "    def updateweight(self):\n",
    "        self.weights_no_bias -= self.learning_rate * self.partial_derivatives\n",
    "        for i in range(self.weights_no_bias.size):\n",
    "            self.weights[i] = self.weights_no_bias[i]\n",
    "        self.weights[-1] -= self.learning_rate * self.delta\n",
    "        print(f\"Weights: {self.weights_no_bias} Bias: {self.weights[-1]}\")\n",
    "        \n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FullyConnected Class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnected:\n",
    "    #initialize with the number of neurons in the layer, their activation,the input size, the leraning rate and a 2d matrix of weights (or else initilize randomly)\n",
    "    def __init__(self, numOfNeurons, activation, input_num, lr, weights=None):\n",
    "        self.neurons = []\n",
    "        self.numOfNeurons = numOfNeurons\n",
    "        self.input_num = input_num\n",
    "        \n",
    "        for i in range(numOfNeurons):\n",
    "            if weights is not None:\n",
    "                self.neurons.append(Neuron(activation, input_num, lr, weights[i]))\n",
    "            else:\n",
    "                self.neurons.append(Neuron(activation, input_num, lr))\n",
    "        \n",
    "        \n",
    "    #calcualte the output of all the neurons in the layer and return a vector with those values (go through the neurons and call the calcualte() method)      \n",
    "    def calculate(self, input):\n",
    "        outputs = []\n",
    "        \n",
    "        for neuron in self.neurons:\n",
    "            output = neuron.calculate(input)\n",
    "            outputs.append(output)\n",
    "            \n",
    "        return np.array(outputs)\n",
    "\n",
    "        \n",
    "            \n",
    "    #given the next layer's w*delta, should run through the neurons calling calcpartialderivative() for each (with the correct value), sum up its ownw*delta, and then update the wieghts (using the updateweight() method). I should return the sum of w*delta.          \n",
    "    def calcwdeltas(self, wtimesdelta):\n",
    "        partial_derivatives = []\n",
    "        i = 0\n",
    "        for neuron in self.neurons:\n",
    "            \n",
    "            partial_derivative = neuron.calcpartialderivative(wtimesdelta[i])\n",
    "            partial_derivatives.append(partial_derivative)\n",
    "            i = i+1\n",
    "        \n",
    "        sum_of_w_delta = np.sum(partial_derivatives, axis=0)\n",
    "        \n",
    "        for neuron in self.neurons:\n",
    "            neuron.updateweight()\n",
    "            \n",
    "        return sum_of_w_delta\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NeuralNetwork Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An entire neural network        \n",
    "class NeuralNetwork:\n",
    "    #initialize with the number of layers, number of neurons in each layer (vector), input size, activation (for each layer), the loss function, the learning rate and a 3d matrix of weights weights (or else initialize randomly)\n",
    "    def __init__(self, numOfLayers, numOfNeurons, inputSize, activation, loss, lr, weights=None):\n",
    "        self.numOfLayers = numOfLayers\n",
    "        self.numOfNeurons = numOfNeurons\n",
    "        self.inputSize = inputSize\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "        self.layers = []\n",
    "        self.weights = np.copy(weights)\n",
    "        for i in range(numOfLayers):       \n",
    "            if weights is not None:\n",
    "                layer_weights = self.weights[i]\n",
    "            else:\n",
    "                layer_weights = None\n",
    "                \n",
    "            layer = FullyConnected(self.numOfNeurons, self.activation, self.inputSize, self.lr, layer_weights)\n",
    "            self.layers.append(layer)\n",
    "\n",
    "    \n",
    "    #Given an input, calculate the output (using the layers calculate() method)\n",
    "    def calculate(self, input):\n",
    "        current_input = input\n",
    "        for layer in self.layers:\n",
    "            current_input = layer.calculate(current_input)\n",
    "        return current_input\n",
    "\n",
    "        \n",
    "    #Given a predicted output and ground truth output simply return the loss (depending on the loss function)\n",
    "    def calculateloss(self, yp, y, loss_function=0):\n",
    "        if loss_function == 0:\n",
    "            loss = np.sum((yp - y)**2) / 2\n",
    "        elif loss_function == 1:\n",
    "            loss = -np.mean(y * np.log(yp) + (1 - y) * np.log(1 - yp))\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss function.\")\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    #Given a predicted output and ground truth output simply return the derivative of the loss (depending on the loss function)        \n",
    "    def lossderiv(self, yp, y):\n",
    "        if self.loss == 0:\n",
    "            return 2 * (yp - y)\n",
    "        elif self.loss == 1:\n",
    "            return yp-y\n",
    "        else:\n",
    "            raise ValueError(\"Invalid loss function\")\n",
    "\n",
    "    \n",
    "    #Given a single input and desired output preform one step of backpropagation (including a forward pass, getting the derivative of the loss, and then calling calcwdeltas for layers with the right values         \n",
    "    def train(self, x, y):\n",
    "        # forward pass to calculate the predicted output\n",
    "        yp = self.calculate(x)\n",
    "\n",
    "        # calculate the loss\n",
    "        loss = self.calculateloss(yp, y)\n",
    "        print(f\"loss: {loss}\")\n",
    "        \n",
    "        # calculate the derivative of the loss\n",
    "        wtimesdelta = self.lossderiv(yp, y)\n",
    "        \n",
    "        # iterate over all layers in reverse order\n",
    "        for i in range(self.numOfLayers-1, -1, -1):\n",
    "            # call calcwdeltas on the current layer with the wtimesdelta from the previous layer\n",
    "            wtimesdelta = self.layers[i].calcwdeltas(wtimesdelta)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main Program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.2983711087600027\n",
      "Weights: [0.35891648 0.40866619] Bias: 0.5307507191857215\n",
      "Weights: [0.51130127 0.56137012] Bias: 0.6190491182582781\n",
      "Weights: [0.14978072 0.19956143] Bias: 0.3456143226552565\n",
      "Weights: [0.24975114 0.29950229] Bias: 0.3450228726473914\n"
     ]
    }
   ],
   "source": [
    "#loading in my example dataset\n",
    "w=np.array([[[.15,.2,.35],[.25,.3,.35]],[[.4,.45,.6],[.5,.55,.6]]])\n",
    "input = np.array([0.05,0.1])\n",
    "desired_output = np.array([0.01,0.99])\n",
    "\n",
    "#training my network on the example data\n",
    "nn = NeuralNetwork(2,2,2,1,1,0.5,w)\n",
    "nn.train(input,desired_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.11360273503808518\n",
      "Weights: [-0.07780089 -0.03008063] Bias: -0.20957274581654903\n",
      "Weights: [0.60968531 0.66021448] Bias: 0.7858125493961037\n",
      "Weights: [0.14946641 0.19893283] Bias: 0.33932828623956146\n",
      "Weights: [0.24910374 0.29820747] Bias: 0.33207473602446647\n",
      "loss: 0.10301489308070919\n",
      "Weights: [-0.10838952 -0.06076505] Bias: -0.26135993857353573\n",
      "Weights: [0.61686648 0.66741814] Bias: 0.797970419691838\n",
      "Weights: [0.14960473 0.19920946] Bias: 0.3420946362611193\n",
      "Weights: [0.24921944 0.29843889] Bias: 0.334388858735437\n",
      "loss: 0.09366305759512084\n",
      "Weights: [-0.13705391 -0.0895137 ] Bias: -0.30983378880752277\n",
      "Weights: [0.62382457 0.67439669] Bias: 0.8097371332335195\n",
      "Weights: [0.14975592 0.19951184] Bias: 0.3451183877685771\n",
      "Weights: [0.24934975 0.2986995 ] Bias: 0.3369950165511306\n",
      "loss: 0.08542775437944537\n",
      "Weights: [-0.16388852 -0.11642243] Bias: -0.35515664955640147\n",
      "Weights: [0.63057145 0.6811622 ] Bias: 0.8211324135905937\n",
      "Weights: [0.14991678 0.19983355] Bias: 0.34833553898333197\n",
      "Weights: [0.24949137 0.29898274] Bias: 0.33982738826116515\n",
      "loss: 0.07818399706319465\n",
      "Weights: [-0.18900545 -0.14160462] Bias: -0.3975221164203325\n",
      "Weights: [0.63711811 0.68772587] Bias: 0.8321748558895291\n",
      "Weights: [0.15008461 0.20016922] Bias: 0.3516921827656973\n",
      "Weights: [0.24964149 0.29928299] Bias: 0.34282989392081836\n",
      "loss: 0.07181033003449867\n",
      "Weights: [-0.21252482 -0.16518155] Bias: -0.43713813330608203\n",
      "Weights: [0.64347475 0.69409807] Bias: 0.8428819845114012\n",
      "Weights: [0.1502572  0.20051439] Bias: 0.35514393292471697\n",
      "Weights: [0.24979779 0.29959558] Bias: 0.3459558178484398\n",
      "loss: 0.06619412838827854\n",
      "Weights: [-0.23456798 -0.1872756 ] Bias: -0.47421498243163007\n",
      "Weights: [0.64965088 0.70028846] Bias: 0.8532703048892523\n",
      "Weights: [0.15043274 0.20086549] Bias: 0.3586548906566284\n",
      "Weights: [0.24995835 0.29991669] Bias: 0.34916691343600337\n",
      "loss: 0.06123414929019435\n",
      "Weights: [-0.25525296 -0.2080057 ] Bias: -0.5089573172605557\n",
      "Weights: [0.65565535 0.70630602] Bias: 0.8633553532117341\n",
      "Weights: [0.15060982 0.20121964] Bias: 0.3621964408893004\n",
      "Weights: [0.25012161 0.30024323] Bias: 0.35243229016516947\n",
      "loss: 0.05684126313126635\n",
      "Weights: [-0.27469177 -0.22748462] Bias: -0.5415592850925256\n",
      "Weights: [0.66149641 0.71215914] Bias: 0.8731517457193609\n",
      "Weights: [0.1507873  0.20157461] Bias: 0.3657460560352034\n",
      "Weights: [0.25028636 0.30057273] Bias: 0.355727271942141\n",
      "loss: 0.052938092419235\n",
      "Weights: [-0.2929888  -0.24581744] Bias: -0.5722018738241768\n",
      "Weights: [0.66718179 0.71785564] Bias: 0.8826732280320895\n",
      "Weights: [0.15096431 0.20192862] Bias: 0.3692862016680515\n",
      "Weights: [0.25045162 0.30090323] Bias: 0.3590323316243632\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "values = [0.5,0.4,0.3,0.2,0.1,0.05]\n",
    "epoch = np.arange(10)\n",
    "\n",
    "# for i in range(len(values)):\n",
    "loss = []\n",
    "aa = NeuralNetwork(2,2,2,1,1,values[0],w)\n",
    "for j in range(10):\n",
    "    nn.train(input,desired_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.2983711087600027\n",
      "loss: 0.2840786382824775\n",
      "loss: 0.26933751884536333\n",
      "loss: 0.2542705784349859\n",
      "loss: 0.239029026356846\n",
      "loss: 0.22378546206100527\n",
      "loss: 0.20872354967739662\n",
      "loss: 0.19402553042707069\n",
      "loss: 0.1798594029161258\n",
      "loss: 0.16636780931179893\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nn = NeuralNetwork(2,2,2,1,1,values[1],w)\n",
    "for j in range(10):\n",
    "    nn.train(input,desired_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "61dc2c637143690a1035320d544cee7d728ae0624d16d6f8c603697c4fc4cfed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
